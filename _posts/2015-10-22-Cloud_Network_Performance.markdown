---
layout: post
title:  "Cloud Network Performance"
date:   2015-10-23
categories: articles,datacenter,data analysis,cluster scheduling
---

Another article that has caught my interest:

_"What we talk about when we talk about Cloud Network Performance"_ by Jeffrey C. Mogul and Lucian Popa, in SigComm CCR 2012

In this paper the authors claim that while there is strong interest in enforcing
network guarantees in the cloud, the form of these guarantees is highly
debated. Here I want to give a short overview of the categorization that is done
in their paper.

The easiest way to implement guarantees on network performance is to reserve a
dedicated cluster per user. This is the typical case of overprovisioning. The
user will not only have to pay for the setup of the cluster, but additionally
for the full reservation time in contrast to the computation time or the resource
usage. (Additionally it is a great baseline for a system research paper, as any
optimization will look great in comparison).

Any approach beyond the baseline has to provide a guarantee on a given level of
granularity. The authors distinguish four levels of granularity:
1. **per-Tenant** Every tenant is given a single aggregate of network usage.
2. **per-VM Hose Model** Bandwidth/Latency are defined per VM.
3. **per-VM Pipe Model** Bandwidth/Latency are defined per VM to VM connection.
4. **per-Flow QoS Model** Bandwidth/Latency limits are upheld for single (e.g.\
   TCP) flows.
It is not clear how many users of cluster application actually have 
enough knowledge about their application to specify its needs in terms of the
per-Flow or Pipe model. On the other hand, there are ways to automatically infer
this knowledge. In case of applications requiring multiple iterations, the
performance can simply be measured (used by Proteus). (As an optimization the measurement could be
done with only a subset of the original data.) For single-shot applications a
model could be used to infer estimates of the network traffic (in case of Hadoop the map and
reduce graph could be used). Optimally the user should only specify time and
cost constraints.

To realise a given performance goal the different approaches use the same set of
knobs. They either optimise the placement of VMs, the scheduling
policies or a combination of both. Approaches that optimize the VM placement
mostly attempt to construct a virtual cluster for each
application (star topology). Approaches that create scheduling policies have to
chose for either implementing them in the core on hardware switches or on the
edge in the hypervisor. Most approaches go for the latter, as the former would
make an upgrade of the (mostly hardware) switches necessary, which would incur
high overhead.

A few notes on the presented systems:

* Distributed Rate Limiting
    * Restricts the amount of traffic generated by one tenant across all sites
      using a single aggregate.
* NetShare
    * Every link shared by more than one tenant is weighted according to predefined
weights
* SecondNet
    * "Virtual Data Center" abstraction with three levels of QoS.
    * "type-0" provides bandwidth guarantee on links between VMs.
    * "type-1" provides bandwidth guarantees for ingress/egress endpoints 
    * other traffic is best-effort.
* SeaWall
    * Employs a hypervisor to hypervisor tunnels with max-min fairness.
* Topology Switching
    * Lets the user specify Bandwidth, Resilience and Isolation for a given
      task.
* Gatekeeper
    * Illusion of single switch, guaranteed (max, min) bandwidth per VM.
    * Hypervisor-based rate limits plus feedback mechanism.
* Oktopus
    * Focuses on performance predictability
    * "Virtual Cluster" VC abstraction all VMs are connected into a single cluster with a given capacity
    * On top of the VC the "Virtual Oversubscribed Cluster" VOC abstraction is
      defined.
    * Uses placement and rate limiting.
* FairCloud
    * Network Proportionality, bandwidth share should be proportional to number of VMs
    * But network proportionality and bandwidth guarantees are mutually exclusive
* ConEx
    * Tenants should have an equal chance to create congestion.
    * Uses ECN support from switches and hypervisor rate limiter.
    * Clients have to express how much congestion they will cause, which is
      difficult for end-users.
* Proteus
    * Temporally-Interleaved Virtual Cluster (TIVC)
    * Profiles running applications
    * Identifies Cost / Performance Trade-Offs
    * Placement algorithm
